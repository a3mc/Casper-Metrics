### Deploy In Production

##### In this document we provide an overview on the production deployment.

The platform core components consist of various units:

* Fail tolerate MySQL cluster
* Master crawler server
* Replication operator servers
* Load balancer
* Static front end
* Mail server

Each of these components is recommended to be deployed separately.

In case of crawling from scratch, the main load is concentrated on Master Crawler Server. We recommend to use dedicated servers for Master Crawler and Replication Operators. Please have a look on [Hardware Requirements](https://github.com/a3mc/Casper-Metrics/blob/master/docs/REQUIREMENTS.md).

Load balancer splits the load generated by the API consumers between Master Crawler Server and Replication Operators. Load balancer points to different endpoints. Each of these endpoints can be configured with a different caching rules. Historical data can have long-living cache, where the real-time data needs to be processed with a micro-cache scenario. The short cache has to be at least less than the current block time, as for example 1 ~ 8 second, and archive cache can be set to a longer time, as for example 1 ~ 6 hours or above. Admin endpoints require real-time communication and should be processed without any cache implementation.

Admin Interface theoretically will never fall under high load and can be served separately, as for example on AWS CloudFront. All admin traffic needs to be encrypted and endpoints covered with SSL certificate.

MySQL cluster communication should be organized with TLS connection as it can contain sensitive data in case of administration.

### Monitoring

Monitoring can be organized with Prometheus and Grafana combo. Grafana can play role of alerting system and watches all dedicated servers together with the MySQL cluster, mail server and TLS encryption status, SSL certificates issuance.

SSL certificates issuance need to be automated and proceed without any downtime whatsoever, which is any way should be standard.

Continuous integration can be implemented across all the modules. So, once a new PR is merged it automatically runs tests and deploys it to the corresponding servers and in most cases relaunches the processes without any downtime.

When there is no need in Replication Operators, if high-load is not planned, Master Crawler can serve all components and provide healthy performance as a crawler, API server, reverse proxy and front-end host. However, we recommend to set up the static components on a cloud such as AWS, for example, for better availability and stability, it will boost front end performance significantly. MySQL can be hosted on the same dedicated server or separately on the cloud, in most cases dev-ops already have MySQL cluster in hands and configuration doesn't take more than just the new database creation with the appropriate security rules.

Master Crawler uses Redis to achieve high speed crawling, as latency there is very important. It's preffered to have Redis installed on the same server with crawler, however, it can be set on separate unit.

Mail server used for administrator operations and its credentials can be configured in `.env` file. See the [example.env](https://github.com/a3mc/Casper-Metrics/blob/master/example.env) for details.

For more details, please see: [Hardware Requirements](https://github.com/a3mc/Casper-Metrics/blob/master/docs/REQUIREMENTS.md).

#### Basic structure of the production setup

![Structure](https://github.com/a3mc/Casper-Metrics/blob/master/docs/infrastructure-map.jpg)
